{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe4a8ef-cd93-4f2b-a889-9933a52c4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要最低限\n",
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import cm\n",
    "\n",
    "X = mnist.download_and_parse_mnist_file(\"train-images-idx3-ubyte.gz\")\n",
    "Y = mnist.download_and_parse_mnist_file(\"train-labels-idx1-ubyte.gz\") \n",
    "\n",
    "#入力層のクラス\n",
    "class InputLayer:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def output(self):\n",
    "        return self.x\n",
    "    \n",
    "# 畳み込み層のクラス\n",
    "class ConvolutionLayer:\n",
    "    def __init__(self, x, filter_mateix, bias, batch, R, s):\n",
    "        self.x = x #batch * ch * dx * dy行列\n",
    "        self.filter_matrix = filter_mateix #フィルター行列だが、R*Rではない。フィルター行列の集合\n",
    "        self.bias = bias #バイアス\n",
    "        self.batch = batch #バッチサイズ\n",
    "        self.R = R #フィルターサイズ\n",
    "        self.s = s #ストライド\n",
    "    def padding(self):\n",
    "        batch, ch, dx, dy = self.x.shape\n",
    "        r = int(self.R/2)\n",
    "        padding = np.zeros((batch, ch, dx + 2*r, dy + 2*r)) \n",
    "        padding[:, :, r:(dx+r), r:(dy+r)] = self.x\n",
    "        return padding\n",
    "    def rerange(self):\n",
    "        batch, ch, dx, dy = self.x.shape\n",
    "        C = self.padding()\n",
    "        C = np.lib.stride_tricks.as_strided(C, shape=(batch, ch, int(dx/self.s), int(dy/self.s), self.R, self.R),\\\n",
    "                                            strides=(C.strides[0], C.strides[1],self.s*C.strides[2], self.s*C.strides[3], C.strides[2], C.strides[3])\n",
    "                                           )\n",
    "        C = C.transpose(1, 4, 5, 0, 2, 3).reshape(ch * self.R * self.R, -1)\n",
    "        return C\n",
    "    def output(self):\n",
    "        W = self.filter_matrix\n",
    "        B = self.bias\n",
    "        return np.dot(W, self.rerange()) + B[:,np.newaxis]\n",
    "    \n",
    "# プーリング層のクラス\n",
    "class PoolingLayer:\n",
    "    def __init__(self, x, batch, R, s):\n",
    "        self.x = x #畳み込み層からの入力\n",
    "        self.batch = batch #バッチサイズ\n",
    "        self.R = R #プーリング層のサイズ\n",
    "        self.s = s #ストライド\n",
    "        self.max_indices = None  # 最大値のインデックスを保持する変数\n",
    "    def output(self):\n",
    "        dx, dy = self.x.shape\n",
    "        newdy = int(np.sqrt(int(dy/self.batch)))\n",
    "        #これにより batch * K * dx * dy配列に変化\n",
    "        tensor_x = np.array(np.array_split(self.x, dy // int(dy/self.batch), axis=1)).reshape(self.batch, dx, newdy, newdy)\n",
    "        #これにpoolingを適用する\n",
    "        d = self.R\n",
    "        n, c, h, w = tensor_x.shape\n",
    "        output_h = (h - d) // self.s + 1 #出力の高さ\n",
    "        output_w = (w - d) // self.s + 1 #出力の幅\n",
    "        pooled_data = tensor_x[:, :, :output_h * self.s, :output_w * self.s].reshape(n, c, output_h, d, output_w, d)\n",
    "        pooled_data = pooled_data.max(axis=(3, 5))\n",
    "        return pooled_data\n",
    "    def max_index(self):\n",
    "        dx, dy = self.x.shape\n",
    "        newdy = int(np.sqrt(int(dy/self.batch)))\n",
    "        #これにより batch * K * dx * dy配列に変化\n",
    "        tensor_x = np.array(np.array_split(self.x, dy // int(dy/self.batch), axis=1)).reshape(self.batch, dx, newdy, newdy)\n",
    "        #これにpoolingを適用する\n",
    "        d = self.R\n",
    "        n, c, h, w = tensor_x.shape\n",
    "        output_h = (h - d) // self.s + 1 #出力の高さ\n",
    "        output_w = (w - d) // self.s + 1 #出力の幅\n",
    "        pooled_data = tensor_x[:, :, :output_h * self.s, :output_w * self.s].reshape(n, c, output_h, d, output_w, d)\n",
    "        # マックスプーリングのマスクを作成、多分これで、インデックスを保持することができているはず、、、\n",
    "        mask = (pooled_data == np.max(pooled_data, axis=(3, 5), keepdims=True))\n",
    "        # mask = np.argwhere(pooled_data == np.max(pooled_data, axis=(3, 5)))\n",
    "        mask = mask.reshape(n, c, output_h*d, output_w*d).astype(int)\n",
    "        return mask\n",
    "\n",
    "# 全結合層のクラス\n",
    "class ConnectionLayer:\n",
    "    def __init__(self, x, w, b):\n",
    "        self.x = x\n",
    "        self.w= w\n",
    "        self.b = b\n",
    "    def output(self):\n",
    "        self.x = np.array(self.x)\n",
    "        self.w = np.array(self.w)\n",
    "        self.b = np.array(self.b)\n",
    "        return np.dot(self.w.T, self.x) + self.b[:, np.newaxis]\n",
    "    def dot_wx(self):\n",
    "        self.x = np.array(self.x)\n",
    "        self.w = np.array(self.w)\n",
    "        return np.dot(self.w.T, self.x)\n",
    "    def dot_b(self):\n",
    "        self.b = np.array(self.b)\n",
    "        return self.b[:, np.newaxis]\n",
    "    def test_output(self):\n",
    "        self.x = np.array(self.x)\n",
    "        self.w = np.array(self.w)\n",
    "        self.b = np.array(self.b)\n",
    "        return np.dot(self.w.T, self.x) + self.b\n",
    "\n",
    "#中間層のクラス(sigmoid, relu)\n",
    "class MiddleLayer:\n",
    "    def __init__(self, x, function):\n",
    "        self.x = x\n",
    "        self.function = function\n",
    "    def output(self):\n",
    "        return self.function(self.x)\n",
    "    \n",
    "# 中間層のクラス（dropout）\n",
    "class Dropout:\n",
    "    def __init__(self, x, rho):\n",
    "        self.x = x\n",
    "        self.rho = rho\n",
    "    def output(self):\n",
    "        rows, cols = self.x.shape\n",
    "        indices = np.arange(rows)\n",
    "        np.random.shuffle(indices)\n",
    "        result_matrix = np.copy(self.x)\n",
    "        for col in range(cols):\n",
    "            selected_indices = indices[:self.rho]\n",
    "            result_matrix[selected_indices, col] = 0\n",
    "        return result_matrix\n",
    "    \n",
    "# 出力層のクラス\n",
    "class OutputLayer:\n",
    "    def __init__(self, x, softmax):\n",
    "        self.x = x\n",
    "        self.softmax = softmax\n",
    "    def output(self):\n",
    "        return self.softmax(self.x)\n",
    "\n",
    "# クロスエントロピーを計算するクラス\n",
    "class CrossEntropy:\n",
    "    def __init__(self, x, onehot):\n",
    "        self.x = x\n",
    "        self.onehot = onehot\n",
    "    def safelog(self):\n",
    "        try:\n",
    "            result = -np.log(self.x)\n",
    "        except RuntimeWarning as e:\n",
    "            result = np.where(self.x <= 0, 100, -np.log(self.x))\n",
    "        return result\n",
    "    def result(self):\n",
    "        loglist = self.safelog()\n",
    "        crossentropy = np.sum(self.onehot * loglist, axis=0)\n",
    "        batch = loglist.shape[1]\n",
    "        return np.sum(crossentropy) / batch\n",
    "\n",
    "# softmaxとクロスエントロピー誤差の逆伝播クラス\n",
    "class ErrorBackCrossEntropy:\n",
    "    def __init__(self, x, onehot):\n",
    "        self.x = x\n",
    "        self.onehot = onehot\n",
    "    def del_en_x(self):\n",
    "        batch = self.x.shape[1]\n",
    "        return (self.x - self.onehot) / batch\n",
    "    \n",
    "# 中間層の逆伝播のクラス\n",
    "# シグモイド関数の逆伝播のクラス\n",
    "class ErrorBackSigmoid:\n",
    "    def __init__(self, y, del_en_y):\n",
    "        self.y = y\n",
    "        self.del_en_y = del_en_y\n",
    "    def del_en_x(self):\n",
    "        new_y = (1 - self.y) * self.y\n",
    "        result = self.del_en_y * new_y\n",
    "        return result\n",
    "    \n",
    "# relu関数の逆伝播のクラス\n",
    "class ErrorBackReLU:\n",
    "    def __init__(self, y, del_en_y):\n",
    "        self.y = y\n",
    "        self.del_en_y = del_en_y\n",
    "    def del_en_x(self):\n",
    "        new_y = np.where(self.y > 0, 1, 0)\n",
    "        result = self.del_en_y * new_y\n",
    "        return result\n",
    "    \n",
    "# dropoutの逆伝播クラス\n",
    "class ErrorBackDropout:\n",
    "    def __init__(self, y, del_en_y):\n",
    "        self.y = y\n",
    "        self.del_en_y = del_en_y\n",
    "    def del_en_x(self):\n",
    "        new_y = np.where(self.y == 0, 0, 1)\n",
    "        result = self.del_en_y * new_y\n",
    "        return result\n",
    "    \n",
    "#正規化層の誤差逆伝播クラス\n",
    "class ErrorBackNormalize:\n",
    "    def __init__(self, x, x_hat, del_en_y, gamma, beta, epsilon, mean, var, batch):\n",
    "        self.x = x\n",
    "        self.x_hat = x_hat\n",
    "        self.del_en_y = del_en_y\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.batch = batch\n",
    "    def del_en_x_hat(self):\n",
    "        return self.del_en_y * self.gamma[:,np.newaxis]\n",
    "    def del_en_var(self):\n",
    "        mult = np.sum(self.del_en_x_hat()*(self.x - self.mean[:, np.newaxis]), axis=1)\n",
    "        return (-1) / 2 * mult *  np.power((self.var + self.epsilon), -1.5 ) \n",
    "    def del_en_mean(self):\n",
    "        e1 = np.sum(self.del_en_x_hat(), axis=1)  * (-1) / np.sqrt(self.var + self.epsilon)\n",
    "        e2 = (-2) * self.del_en_var() * (np.sum(self.x, axis=1)/self.batch - self.mean)\n",
    "        return e1 + e2\n",
    "    def del_en_x(self):\n",
    "        e1 = self.del_en_x_hat() / np.sqrt(self.var + self.epsilon)[:,np.newaxis]\n",
    "        e2 = 2 / self.batch * (self.x - self.mean[:,np.newaxis]) * self.del_en_var()[:,np.newaxis]\n",
    "        e3 = self.del_en_mean() / self.batch\n",
    "        return e1 + e2 + e3[:,np.newaxis]\n",
    "    def del_en_gamma(self):\n",
    "        return np.sum(self.del_en_y * self.x, axis=1)\n",
    "    def del_en_beta(self):\n",
    "        return np.sum(self.del_en_y, axis=1)\n",
    "    \n",
    "# 全結合層の逆伝播クラス\n",
    "class ErrorBackConnect:\n",
    "    def __init__(self, x, del_en_y, w):\n",
    "        self.x = x\n",
    "        self.del_en_y = del_en_y\n",
    "        self.w = w\n",
    "    def del_en_x(self):\n",
    "        return np.dot(self.w, self.del_en_y)\n",
    "    def del_en_w(self):\n",
    "        return np.dot(self.del_en_y, self.x.T)\n",
    "    def del_en_b(self):\n",
    "        return np.sum(self.del_en_y, axis=1)\n",
    "    \n",
    "# プーリング層の誤差逆伝播クラス\n",
    "class ErrorBackPooling:\n",
    "    def __init__(self, x, y, del_en_y, K, batch, max_index, R, R_conv):\n",
    "        self.x = x #プーリング層の入力\n",
    "        self.y = y #プーリング層の出力\n",
    "        self.del_en_y = del_en_y\n",
    "        self.K = K #フィルタ数\n",
    "        self.batch = batch\n",
    "        self.max_index = max_index #プーリング層で抽出された要素は1, そうでない要素は0となっている配列\n",
    "        self.R = R #プーリングのサイズ\n",
    "        self.R_conv = R_conv\n",
    "    def reshape_del(self, m):\n",
    "        self.m = m\n",
    "        z, batch = self.m.shape\n",
    "        dx = int(np.sqrt(int(z/self.K)))\n",
    "        #これにより、batch*K*dx*dx配列に変化\n",
    "        tensor_del = self.m.T.reshape(batch, self.K, dx, dx)\n",
    "        return tensor_del\n",
    "    def reshape_x(self, n):\n",
    "        self.n = n\n",
    "        dx, dy = self.n.shape\n",
    "        newdy = int(np.sqrt(int(dy/self.batch)))\n",
    "        #これにより batch * K * dx * dy配列に変化\n",
    "        tensor_x = np.array(np.array_split(self.n, dy // int(dy/self.batch), axis=1)).reshape(self.batch, dx, newdy, newdy)\n",
    "        #これにpoolingを適用する\n",
    "        return tensor_x\n",
    "    #こちらは、pooling層2の誤差逆伝播に限定することにする\n",
    "    def del_en_x(self):\n",
    "        del_en_y_reshaped = self.reshape_del(self.del_en_y)\n",
    "        b, k, dx, dy =  self.reshape_x(self.x).shape\n",
    "        expanded_del = del_en_y_reshaped.repeat(self.R, axis=2).repeat(self.R, axis=3)\n",
    "        expanded_del = expanded_del.reshape(b, k, dx, dy)\n",
    "        C = expanded_del * self.max_index\n",
    "        return C.transpose(1, 0, 2, 3).reshape(k, -1)\n",
    "    #こちらはpooling層1の誤差逆伝播に限定\n",
    "    def del_en_x_sub(self):\n",
    "        # b, k, dx, dy = self.y.shape\n",
    "        b, k, h, w = self.reshape_x(self.x).shape\n",
    "        # expanded_del = self.del_en_y[np.arange(self.del_en_y.shape[0]) % 9 == 4]\n",
    "        # expanded_del = self.reshape_del(expanded_del)\n",
    "        # expanded_del = expanded_del.repeat(self.R, axis=2).repeat(self.R, axis=3)\n",
    "        # expanded_del = expanded_del.reshape(b, k, w, h)\n",
    "        # dx, dy = self.x.shape\n",
    "        expanded_del = col2im(self.del_en_y, self.y.shape, 3, 3, 1, 1)\n",
    "        expanded_del = expanded_del.repeat(self.R, axis=2).repeat(self.R, axis=3)\n",
    "        expanded_del = expanded_del.reshape(b, k, w, h)\n",
    "        \n",
    "        C = expanded_del * self.max_index\n",
    "        return C.transpose(1, 0, 2, 3).reshape(k, -1)\n",
    "    \n",
    "# 畳み込み層の誤差逆伝播クラス\n",
    "class ErrorBackConvolution:\n",
    "    def __init__(self, x, del_en_y, w):\n",
    "        self.x = x\n",
    "        self.del_en_y = del_en_y\n",
    "        self.w = w\n",
    "    def del_en_x(self):\n",
    "        return np.dot(self.w.T, self.del_en_y)\n",
    "    def del_en_w(self):\n",
    "        return np.dot(self.del_en_y, self.x.T)\n",
    "    def del_en_b(self):\n",
    "        return np.sum(self.del_en_y, axis=1)\n",
    "    def del_en_x_sub(self):\n",
    "        del_y = np.dot(self.w.T, self.del_en_y)\n",
    "        expanded_del = del_y[np.arange(del_y.shape[0]) % 9 == 4]\n",
    "        return expanded_del\n",
    "    \n",
    "# シグモイド関数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ソフトマックス関数\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis = 0)\n",
    "    return np.exp(np.array(x)) / np.sum(np.exp(np.array(x)), axis=0)\n",
    "\n",
    "# リストの一致率を示す関数\n",
    "def calculate_similarity(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"リストの長さが異なります\")\n",
    "\n",
    "    # 一致した要素の数を数える\n",
    "    match_count = sum(1 for a, b in zip(list1, list2) if a == b)\n",
    "\n",
    "    # 一致率を計算\n",
    "    similarity = (match_count / len(list1)) * 100  # パーセントで表示\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# ReLU関数\n",
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3117b0-773c-4e37-bc52-8e25efc13617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均のクロスエントロピー1.0865384597553776\n",
      "正解率:13.0\n",
      "テスト開始\n"
     ]
    }
   ],
   "source": [
    "#Mnistコンテスト本番ファイル（訓練用）\n",
    "#畳み込み→活性化→プーリング→バッチ正規化→活性化→全結合→ソフトマックス\n",
    "#畳み込み層のフィルタサイズは3*3、ストライドは1\n",
    "#活性化関数には全てReLUを用いる\n",
    "#プーリングはフィルタサイズ2*2、ストライドは2\n",
    "\n",
    "np.random.seed(673)\n",
    "#チャンネル数\n",
    "ch = 1\n",
    "#畳み込み層のフィルタサイズ\n",
    "R = 3\n",
    "#プーリング層のフィルタのサイズ\n",
    "R_pool = 2\n",
    "#フィルタの数\n",
    "K = 30\n",
    "#畳み込み層のストライド\n",
    "s = 1\n",
    "#プーリング層のストライド\n",
    "s_pool = 2\n",
    "#バッチ数\n",
    "batch = 100\n",
    "#2回目のプーリングが終了した時点でのデータサイズ\n",
    "d = K * 14 * 14\n",
    "#クラス数\n",
    "C = 10\n",
    "#フィルター行列\n",
    "filter_matrix = np.array(np.random.normal(0, 1/np.sqrt(d), (K, R*R*ch)))\n",
    "#バイアス項\n",
    "bias_vector = np.array(np.random.normal(0, 1/np.sqrt(d), K))\n",
    "#全結合層の行列\n",
    "W_matrix = np.array(np.random.normal(0, 1/np.sqrt(d), (d, C)))\n",
    "#全結合層のベクトル\n",
    "b_vector = np.array(np.random.normal(0, 1/np.sqrt(d), C))\n",
    "#正規化パラメータの初期値\n",
    "gamma_init = np.ones(d)\n",
    "beta_init = np.zeros(d)\n",
    "mean_init = np.zeros(d)\n",
    "var_init = np.zeros(d)\n",
    "\n",
    "def mnistcontest_train(batch, W_init, b_init, gamma_init, beta_init, filter_init, bias_init, mean_init, var_init):\n",
    "    #変数を初期化\n",
    "    W = W_init\n",
    "    b = b_init\n",
    "    gamma = gamma_init\n",
    "    beta = beta_init\n",
    "    mean_cross_entropy = 0\n",
    "    mean_mean = np.zeros(d)\n",
    "    mean_var = np.zeros(d)\n",
    "    filter_matrix = filter_init\n",
    "    bias = bias_init\n",
    "    #バッチ正規化前エポックにおけるバッチの平均と分散\n",
    "    mean = mean_init\n",
    "    var = var_init\n",
    "    #学習率\n",
    "    eta = 0.01\n",
    "    #イプシロンの値\n",
    "    epsilon = 1.0e-15\n",
    "    #0~50000までを重複なく取得\n",
    "    sampled_data =  np.random.choice(np.arange(0, 50000), size=(int(50000/batch), batch), replace=False)\n",
    "    i = len(sampled_data)-1\n",
    "    X_train = X[:, np.newaxis, :, :]\n",
    "    while(i > 0):\n",
    "        batch_index = sampled_data[i] \n",
    "        #正規化\n",
    "        batch_matrix = np.array(X_train[batch_index])/255 \n",
    "        #正解データ\n",
    "        correct_labels = Y[batch_index] \n",
    "        #one-hot\n",
    "        one_hot = np.array([np.eye(10)[y] for y in correct_labels]).T\n",
    "        #入力\n",
    "        input_layer = InputLayer(batch_matrix)\n",
    "        #畳み込み\n",
    "        convolution_layer = ConvolutionLayer(input_layer.output(), filter_matrix, bias, batch, R, s)\n",
    "        #活性化1\n",
    "        activate_layer1 = MiddleLayer(convolution_layer.output(), relu)\n",
    "        #プーリング\n",
    "        pooling_layer = PoolingLayer(activate_layer1.output(), batch, R_pool, s_pool)\n",
    "        #プーリングした多次元配列を行列の形に整形\n",
    "        preaffine = pooling_layer.output().reshape(batch, -1).T\n",
    "        #バッチ正規化\n",
    "        row_means = np.mean(preaffine, axis=1)\n",
    "        mean_mean += row_means\n",
    "        row_variances = np.var(preaffine, axis=1)\n",
    "        mean_var += row_variances\n",
    "        stdiv = np.sqrt(row_variances + epsilon)\n",
    "        x_hat = (preaffine - row_means[:, np.newaxis]) / stdiv[:, np.newaxis]\n",
    "        normalized_matrix = gamma[:,np.newaxis] * x_hat + beta[:,np.newaxis]\n",
    "        #活性化2\n",
    "        activate_layer2 = MiddleLayer(normalized_matrix, relu)\n",
    "        #全結合\n",
    "        connection_layer = ConnectionLayer(activate_layer2.output(), W, b)\n",
    "        #ソフトマックス\n",
    "        output_layer = OutputLayer(connection_layer.output(), softmax) \n",
    "        #クロスエントロピー誤差伝播\n",
    "        errorback_crossentropy = ErrorBackCrossEntropy(output_layer.output(), one_hot)\n",
    "        #全結合層の逆伝播\n",
    "        errorback_connect = ErrorBackConnect(activate_layer2.output(), errorback_crossentropy.del_en_x(), W)\n",
    "        #活性化2の誤差逆伝播\n",
    "        errorback_relu2 = ErrorBackReLU(normalized_matrix, errorback_connect.del_en_x())\n",
    "        #バッチ正規化層の誤差逆伝播\n",
    "        errorback_normalize = ErrorBackNormalize(preaffine, x_hat, errorback_relu2.del_en_x(), \n",
    "                                                 gamma, beta, epsilon, row_means, row_variances, batch)\n",
    "        #プーリングの誤差逆伝播\n",
    "        errorback_pooling = ErrorBackPooling(activate_layer1.output(), pooling_layer.output(), errorback_normalize.del_en_x(), \n",
    "                                             K, batch, pooling_layer.max_index(), R_pool, R) \n",
    "        #活性化層1の誤差逆伝播\n",
    "        errorback_relu1 = ErrorBackReLU(convolution_layer.output(), errorback_pooling.del_en_x())\n",
    "        #畳み込み層の誤差逆伝播\n",
    "        errorback_convolution = ErrorBackConvolution(convolution_layer.rerange(), errorback_relu1.del_en_x(), filter_matrix)\n",
    "        #変数を更新\n",
    "        W = W - eta * errorback_connect.del_en_w().T\n",
    "        b = b - eta * errorback_connect.del_en_b()\n",
    "        gamma = gamma - eta * errorback_normalize.del_en_gamma()\n",
    "        beta = beta - eta * errorback_normalize.del_en_beta()\n",
    "        filter_matrix = filter_matrix - eta * errorback_convolution.del_en_w()\n",
    "        bias = bias - eta * errorback_convolution.del_en_b()\n",
    "        i -= 1\n",
    "        crossentropy = CrossEntropy(output_layer.output(), one_hot)\n",
    "        mean_cross_entropy += crossentropy.result()\n",
    "        # print(crossentropy.result())\n",
    "        if(i==0):\n",
    "            test_mean = mean_mean / len(sampled_data)\n",
    "            test_var = mean_var / len(sampled_data)\n",
    "            # test_mean = (test_mean + mean)/2\n",
    "            # test_var = (test_var + var)/2\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/W\", W)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/b\", b)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/gamma\", gamma)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/beta\", beta)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/test_mean\", test_mean)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/test_var\", test_var)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/filter_matrix\", filter_matrix)\n",
    "            np.save(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/bias\", bias)\n",
    "            print(\"平均のクロスエントロピー\"+str(mean_cross_entropy/len(sampled_data)))\n",
    "            print(\"正解率:\" + str(calculate_similarity(np.argmax(output_layer.output(), axis=0).tolist(), correct_labels)))\n",
    "            print(\"テスト開始\")\n",
    "            test_matrix = X_train[50000:]\n",
    "            testbatch, k, dx, dy = test_matrix.shape\n",
    "            #正規化\n",
    "            test_matrix = np.array(test_matrix) / 255\n",
    "            answer_labels = Y[50000:]\n",
    "            #入力層\n",
    "            test_input_layer = InputLayer(test_matrix)\n",
    "            #畳み込み\n",
    "            test_convolution_layer = ConvolutionLayer(test_input_layer.output(), filter_matrix, bias, testbatch, R, s)\n",
    "            #活性化1\n",
    "            test_activate_layer1 = MiddleLayer(test_convolution_layer.output(), relu)\n",
    "            #プーリング\n",
    "            test_pooling_layer = PoolingLayer(test_activate_layer1.output(), testbatch, R_pool, s_pool)\n",
    "            #行列に整形、正規化\n",
    "            test_preaffine = test_pooling_layer.output().reshape(testbatch, -1).T\n",
    "            e1 = (gamma / np.sqrt(test_var + epsilon))[:,np.newaxis] * test_preaffine\n",
    "            e2 = beta - gamma * test_mean / np.sqrt(test_var + epsilon)\n",
    "            normalized_output = e1 + e2[:,np.newaxis]\n",
    "            #活性化2\n",
    "            test_activate_layer2 = MiddleLayer(normalized_output, relu)\n",
    "            #全結合\n",
    "            test_connection_layer = ConnectionLayer(test_activate_layer2.output(), W, b)\n",
    "            #ソフトマックス\n",
    "            output_layer_test = OutputLayer(test_connection_layer.output(), softmax)\n",
    "            print(\"テスト正解率:\" + str(calculate_similarity(np.argmax(output_layer_test.output(), axis=0).tolist(), answer_labels)))\n",
    "            \n",
    "\n",
    "            \n",
    "mnistcontest_train(100, W_matrix, b_vector, gamma_init, beta_init, filter_matrix, bias_vector, mean_init, var_init)\n",
    "# j = 10\n",
    "# while(j > 0):\n",
    "#     W = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/W.npy\")\n",
    "#     b = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/b.npy\")\n",
    "#     gamma = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/gamma.npy\")\n",
    "#     beta = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/beta.npy\")\n",
    "#     filter_init = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/filter_matrix.npy\")\n",
    "#     bias = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/bias.npy\")\n",
    "#     mean = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/test_mean.npy\")\n",
    "#     var = np.load(\"/Users/mst923/OneDrive - Kyoto University/3年後期/計算機科学実験4/画像処理/mnist_test/test_var.npy\")\n",
    "#     mnistcontest_train(100, W, b, gamma, beta, filter_init, bias, mean, var)\n",
    "#     j -= 1\n",
    "#     print(\"epoch残り\"+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607637c-d1b5-4c7e-9c94-d79a2731feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mnistコンテスト本番ファイル（テスト用）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
